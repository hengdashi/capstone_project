@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article {Johnsone2011362118,
	author = {Johnson, Paul A. and Rouet-Leduc, Bertrand and Pyrak-Nolte, Laura J. and Beroza, Gregory C. and Marone, Chris J. and Hulbert, Claudia and Howard, Addison and Singer, Philipp and Gordeev, Dmitry and Karaflos, Dimosthenis and Levinson, Corey J. and Pfeiffer, Pascal and Puk, Kin Ming and Reade, Walter},
	title = {Laboratory earthquake forecasting: A machine learning competition},
	volume = {118},
	number = {5},
	elocation-id = {e2011362118},
	year = {2021},
	doi = {10.1073/pnas.2011362118},
	publisher = {National Academy of Sciences},
	abstract = {Earthquake prediction, the long-sought holy grail of earthquake science, continues to confound Earth scientists. Could we make advances by crowdsourcing, drawing from the vast knowledge and creativity of the machine learning (ML) community? We used Google{\textquoteright}s ML competition platform, Kaggle, to engage the worldwide ML community with a competition to develop and improve data analysis approaches on a forecasting problem that uses laboratory earthquake data. The competitors were tasked with predicting the time remaining before the next earthquake of successive laboratory quake events, based on only a small portion of the laboratory seismic data. The more than 4,500 participating teams created and shared more than 400 computer programs in openly accessible notebooks. Complementing the now well-known features of seismic data that map to fault criticality in the laboratory, the winning teams employed unexpected strategies based on rescaling failure times as a fraction of the seismic cycle and comparing input distribution of training and testing data. In addition to yielding scientific insights into fault processes in the laboratory and their relation with the evolution of the statistical properties of the associated seismic data, the competition serves as a pedagogical tool for teaching ML in geophysics. The approach may provide a model for other competitions in geosciences or other domains of study to help engage the ML community on problems of significance.The competition dataset and binary data have been deposited in Kaggle (https://www.kaggle.com/c/LANL-Earthquake-Prediction/data).},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/5/e2011362118},
	eprint = {https://www.pnas.org/content/118/5/e2011362118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{REN2021100178,
title = {Deep Learning-Based Weather Prediction: A Survey},
journal = {Big Data Research},
volume = {23},
pages = {100178},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100178},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300460},
author = {Xiaoli Ren and Xiaoyong Li and Kaijun Ren and Junqiang Song and Zichen Xu and Kefeng Deng and Xiang Wang},
keywords = {Deep learning, Big meteorological data, Weather forecasting, Spatio-temporal feature, Time series},
abstract = {Weather forecasting plays a fundamental role in the early warning of weather impacts on various aspects of human livelihood. For instance, weather forecasting provides decision making support for autonomous vehicles to reduce traffic accidents and congestions, which completely depend on the sensing and predicting of external environmental factors such as rainfall, air visibility and so on. Accurate and timely weather prediction has always been the goal of meteorological scientists. However, the conventional theory-driven numerical weather prediction (NWP) methods face many challenges, such as incomplete understanding of physical mechanisms, difficulties in obtaining useful knowledge from the deluge of observation data, and the requirement of powerful computing resources. With the successful application of data-driven deep learning method in various fields, such as computer vision, speech recognition, and time series prediction, it has been proven that deep learning method can effectively mine the temporal and spatial features from the spatio-temporal data. Meteorological data is a typical big geospatial data. Deep learning-based weather prediction (DLWP) is expected to be a strong supplement to the conventional method. At present, many researchers have tried to introduce data-driven deep learning into weather forecasting, and have achieved some preliminary results. In this paper, we survey the state-of-the-art studies of deep learning-based weather forecasting, in the aspects of the design of neural network (NN) architectures, spatial and temporal scales, as well as the datasets and benchmarks. Then we analyze the advantages and disadvantages of DLWP by comparing it with the conventional NWP, and summarize the potential future research topics of DLWP.}
}

@misc{wu2020deep,
      title={Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case}, 
      author={Neo Wu and Bradley Green and Xue Ben and Shawn O'Banion},
      year={2020},
      eprint={2001.08317},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{alphago2016,
title	= {Mastering the game of Go with deep neural networks and tree search},
author	= {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
year	= {2016},
URL	= {http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html},
journal	= {Nature},
pages	= {484--503},
volume	= {529}
}

@inproceedings{Kipf:2016tc,
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured
data that is based on an efficient variant of convolutional neural networks which
operate directly on graphs. We motivate the choice of our convolutional architecture
via a localized first-order approximation of spectral graph convolutions.
Our model scales linearly in the number of graph edges and learns hidden layer
representations that encode both local graph structure and features of nodes. In
a number of experiments on citation networks and on a knowledge graph dataset
we demonstrate that our approach outperforms related methods by a significant
margin.},
  added-at = {2020-07-15T00:50:01.000+0200},
  author = {Kipf, Thomas N. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/271ee5be8cafc25d7a3869bcb49fc5c3c/twagener},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations},
  interhash = {54b65044b71f10c31476ed76422ab85d},
  intrahash = {71ee5be8cafc25d7a3869bcb49fc5c3c},
  keywords = {},
  location = {Palais des Congr{\`e}s Neptune, Toulon, France},
  series = {ICLR '17},
  timestamp = {2020-07-15T00:50:01.000+0200},
  title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  venue = {ICLR},
  year = 2017
}


@misc{dosovitskiy2020image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2020},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{shi2021masked,
      title={Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification},
      author={Yunsheng Shi and Zhengjie Huang and shikun feng and Hui Zhong and Wenjin Wang and Yu Sun},
      year={2021},
      url={https://openreview.net/forum?id=B9t708KMr9d}
}


@inproceedings{vaswani2017attention,
               author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
               booktitle = {Advances in Neural Information Processing Systems},
               editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
               pages = {},
               publisher = {Curran Associates, Inc.},
               title = {Attention is All you Need},
               url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
               volume = {30},
               year = {2017}
}


@misc{oord2016wavenet,
      title={WaveNet: A Generative Model for Raw Audio}, 
      author={Aaron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
      year={2016},
      eprint={1609.03499},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}


@inbook{10.5555/303568.303704,
        author = {LeCun, Yann and Bengio, Yoshua},
        title = {Convolutional Networks for Images, Speech, and Time Series},
        year = {1998},
        isbn = {0262511029},
        publisher = {MIT Press},
        address = {Cambridge, MA, USA},
        booktitle = {The Handbook of Brain Theory and Neural Networks},
        pages = {255–258},
        numpages = {4}
}


@inbook{10.5555/65669.104451,
        author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
        title = {Learning Representations by Back-Propagating Errors},
        year = {1988},
        isbn = {0262010976},
        publisher = {MIT Press},
        address = {Cambridge, MA, USA},
        booktitle = {Neurocomputing: Foundations of Research},
        pages = {696–699},
        numpages = {4}
}


@article{10.1162/neco.1997.9.8.1735,
         author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
         title = {Long Short-Term Memory},
         year = {1997},
         issue_date = {November 15, 1997},
         publisher = {MIT Press},
         address = {Cambridge, MA, USA},
         volume = {9},
         number = {8},
         issn = {0899-7667},
         url = {https://doi.org/10.1162/neco.1997.9.8.1735},
         doi = {10.1162/neco.1997.9.8.1735},
         abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
         journal = {Neural Comput.},
         month = nov,
         pages = {1735–1780},
         numpages = {46}
}


@inproceedings{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{haoyietal-informer-2021,
  author    = {Haoyi Zhou and
               Shanghang Zhang and
               Jieqi Peng and
               Shuai Zhang and
               Jianxin Li and
               Hui Xiong and
               Wancai Zhang},
  title     = {Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  booktitle = {The Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021},
  pages     = {online},
  publisher = {{AAAI} Press},
  year      = {2021},
}


@inproceedings{luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}


@article{child2019sparsetransformer,
  title={Generating Long Sequences with Sparse Transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={URL https://openai.com/blog/sparse-transformers},
  year={2019}
}



@inproceedings{NEURIPS2019_6775a063,
 author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting},
 url = {https://proceedings.neurips.cc/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}


@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
